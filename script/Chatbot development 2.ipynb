{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc402a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefe7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "data = pd.read_csv(r\"C:\\Users\\60122\\Downloads\\FYP\\dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the intent\n",
    "data = data[['cancel_order', 'check_refund_policy', 'complaint', 'contact_customer_service', 'contact_human_agent', 'delete_account', 'delivery_options', 'delivery_period', 'edit_account', 'get_refund', 'recover_password','review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b78579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add intent to the list \n",
    "\n",
    "intents = []\n",
    "for num,intent in enumerate(data):\n",
    "    intents.append(intent)\n",
    "\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three lists to store corpus and intent\n",
    "words = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "# perform tokenization using Regex (w+ matches any words except newline)\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "#loop through dataset\n",
    "for column in data:\n",
    "    \n",
    "    for row in data[column]:\n",
    "        #convert r to string and lowercase it \n",
    "        \n",
    "        r = str(row)\n",
    "        r = r.lower()\n",
    "        # if r is nan , skip it , otherwise add both data and intent into list \n",
    "        if r != 'nan' :\n",
    "            tokens = nltk.word_tokenize(r)\n",
    "            #add each tokens to list \n",
    "            words.extend(tokens)\n",
    "            docs_x.append(tokens)\n",
    "            docs_y.append(column)\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f646a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "for column in data:\n",
    "    \n",
    "    for row in data[column]:\n",
    "        row = str(row)\n",
    "        temp.append(tt.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca17e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a stopwords set from nltk\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = LancasterStemmer()\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in stopwords]\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_empty = [0 for _ in range(len(labels))]\n",
    "out_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7008ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = [stemmer.stem(w) for w in doc]\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = np.array(training)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hfTokenizer(text):\n",
    "    start = (datetime.now())\n",
    "    print(tokenizer.encode(text).tokens)\n",
    "    end = (datetime.now())\n",
    "    print(\"Time taken - {} microseconds\".format((end-start).microseconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (BertWordPieceTokenizer)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "sen = \"i done want my online account and wanna remove it\"\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(sen)\n",
    "print(tt.tokenize(sen))\n",
    "nltk_tokens\n",
    "#tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdc24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281306b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f357bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
